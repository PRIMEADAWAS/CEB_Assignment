{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRIMEADAWAS/CEB_Assignment/blob/main/LlamaIndex101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OSJOuyTEQ3U",
        "outputId": "e57e2dc4-0f47-4fa8-d0a0-0a155af905ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=sk-YOUR_API_KEY\n"
          ]
        }
      ],
      "source": [
        "%set_env OPENAI_API_KEY=YOUR_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaxmzHxGEQ3Y"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQo7MHZhEQ3Y"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-llms-openai\n",
        "!pip install llama-index-llms-openai-like\n",
        "!pip install llama-index-llms-huggingface"
      ],
      "metadata": {
        "id": "-gRPKL2EFsVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "rX7_zRuoUgXm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlSCmLQgEQ3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef6b9aa-d032-4f73-9e6b-bc27d6601d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk is a billionaire entrepreneur and CEO of multiple companies, including Tesla, SpaceX, Neuralink, and The Boring Company. He is known for his ambitious vision for the future, including colonizing Mars, developing sustainable energy solutions, and advancing artificial intelligence. Musk is also known for his outspoken and sometimes controversial statements on social media.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model = 'gpt-3.5-turbo',temperature=0.1,max_tokens=100)\n",
        "response = llm.complete(\"who is elon musk\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging face"
      ],
      "metadata": {
        "id": "Qs6tnBL9N_uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_API_KEY\""
      ],
      "metadata": {
        "id": "Eh5nFJR4OFU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import (\n",
        "    HuggingFaceInferenceAPI,\n",
        "    HuggingFaceLLM,\n",
        ")\n",
        "HF_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")"
      ],
      "metadata": {
        "id": "oYBKWY1SNdJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remotely_run = HuggingFaceInferenceAPI(\n",
        "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=HF_TOKEN\n",
        ")\n",
        "# remotely_run_anon = HuggingFaceInferenceAPI(\n",
        "#     model_name=\"HuggingFaceH4/zephyr-7b-alpha\"\n",
        "# )"
      ],
      "metadata": {
        "id": "3aPBlUPkOQOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f08163-e19c-4196-97d9-a740572a1bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion_response = remotely_run.complete(\"To infinity, and\")\n",
        "print(completion_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUrrltP4OhAo",
        "outputId": "4847a00f-1aaa-4c95-b7fe-9dd479d37b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " beyond!\n",
            "\n",
            "The Toy Story franchise has been a beloved part of pop culture for over two decades, and it's not slowing down anytime soon. The latest installment, Toy Story 4, is set to hit theaters this summer, and it's already generating buzz.\n",
            "\n",
            "The movie follows the adventures of Woody, Buzz, and the gang as they embark on a new adventure with a new toy, Forky. The trailer for the movie has been released, and it's already getting fans excited for the film.\n",
            "\n",
            "One of the most exciting things about Toy Story 4 is the return of some beloved characters. Bo Peep, who was last seen in Toy Story 2, is back and looking better than ever. She's now a modern, independent woman, and her new look has been getting a lot of attention.\n",
            "\n",
            "Another exciting addition to the movie is the introduction of new characters, including Forky, who is voiced by Tony Hale. Forky is a spork with a popsicle stick for a handle, and he's not exactly thrilled about being a toy.\n",
            "\n",
            "The trailer for Toy Story 4 has been viewed over 10 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RVWCnPXEQ3c"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-embeddings-instructor"
      ],
      "metadata": {
        "id": "KyANA1QwRDTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkPepgHrEQ3d",
        "outputId": "d67481e1-e31b-4a4b-daa9-519a498a5727",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n",
            "[0.03607286512851715, -0.0005120203131809831, -0.03582380712032318, -0.03432123363018036, 0.010324924252927303, -0.02194463461637497, -0.044648975133895874, 0.04291066527366638, 0.04395976662635803, -0.015890754759311676]\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\").get_text_embedding(\"Hello World!\")\n",
        "print(len(embed_model))\n",
        "print(embed_model[:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
        "print(len(embeddings))\n",
        "print(embeddings[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOuXX7fLRPAG",
        "outputId": "7e74bf9f-244a-4637-dbd5-829f544f4a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384\n",
            "[-0.003275663824751973, -0.011690725572407246, 0.04155917093157768, -0.03814810514450073, 0.02418305166065693, 0.013644285500049591, 0.0111179044470191, 0.04811961576342583, 0.02140955626964569, 0.014174910262227058]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading"
      ],
      "metadata": {
        "id": "DOttCBP5pNRi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNqkdDB3FCyS"
      },
      "source": [
        "## Reader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLjTY340FCyW"
      },
      "source": [
        "### HTMLNodeParser\n",
        "can set specific tags"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```html\n",
        "<html>\n",
        "  <head>\n",
        "      <title>Mockup Data</title>\n",
        "  </head>\n",
        "  <body>  \n",
        "      <h1>Say Hello</h1>\n",
        "      <h2>Say World</h2>\n",
        "      <h3>Say !</h3>\n",
        "      <pre>\n",
        "          <code>\n",
        "              print(\"Hello World!\")\n",
        "          </code>\n",
        "      </pre>\n",
        "  </body>\n",
        "</html>\n"
      ],
      "metadata": {
        "id": "1Husgb0y3JbR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV8apm3qFCyW",
        "outputId": "d50dab95-b840-45be-db7a-392763d83232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 Say Hello\n",
            "1 Say World\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.node_parser import HTMLNodeParser\n",
        "\n",
        "html_docs = FlatReader().load_data(Path(\"./MockupData/data.html\"))\n",
        "parser = HTMLNodeParser(tags=['h1','h2'])  # optional list of tags\n",
        "# parser = HTMLNodeParser()\n",
        "html_nodes = parser.get_nodes_from_documents(html_docs)\n",
        "\n",
        "for idx,i in enumerate(html_nodes) :\n",
        "    print(idx,i.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-q0imAFCyX"
      },
      "source": [
        "### JSONNodeParser\n",
        "nested cannot read"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```json\n",
        "{\n",
        "    \"h1\" : \"Say Hello\",\n",
        "    \"h2\" : \"Say World\",\n",
        "    \"h3\" : \"Say !\",\n",
        "    \"code\" : \"print(\\\"Hello World!\\\")\",\n",
        "    \"nested\" : {\n",
        "        \"nest_h1\" : \"Nested Hello\",\n",
        "        \"nest_h2\" : \"Nested World\",\n",
        "        \"nest_h3\" : \"Nested !\"\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "zrgB5Izg3UFg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4jHR81LFCyX",
        "outputId": "22c9680e-af00-4505-de5b-e567c4e29fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 Say Hello\n",
            "1 Say World\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.node_parser import JSONNodeParser\n",
        "\n",
        "json_docs = FlatReader().load_data(Path(\"./MockupData/data.json\"))\n",
        "parser = JSONNodeParser()\n",
        "json_nodes = parser.get_nodes_from_documents(json_docs)\n",
        "for idx,i in enumerate(html_nodes) :\n",
        "    print(idx,i.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-ML6PJBFCyY"
      },
      "source": [
        "### MarkdownNodeParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHN_UuzsFCyY",
        "outputId": "e5ff0ef8-001e-43b4-d98c-7949c99dbb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 H1 Say Hello\n",
            "1 H2 Say World\n",
            "2 H3 Say !\n",
            "\n",
            "```\n",
            "print(\"Hello World!\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "\n",
        "markdown_docs = FlatReader().load_data(Path(\"./MockupData/data.md\"))\n",
        "parser = MarkdownNodeParser()\n",
        "markdown_nodes = parser.get_nodes_from_documents(markdown_docs)\n",
        "for idx,i in enumerate(markdown_nodes) :\n",
        "    print(idx,i.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkkR2foL1Zz3"
      },
      "source": [
        "### SimpleFileNodeParser\n",
        "auto detect file type by extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "753c029d-f0f4-4bd6-8a6e-3e039bba006b",
        "id": "S5mCmHRn1Zz4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maatick/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 H1 Say Hello\n",
            "1 H2 Say World\n",
            "2 H3 Say !\n",
            "\n",
            "```\n",
            "print(\"Hello World!\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.node_parser import SimpleFileNodeParser\n",
        "from llama_index.readers.file import FlatReader\n",
        "from pathlib import Path\n",
        "\n",
        "md_docs = FlatReader().load_data(Path(\"./MockupData/data.md\"))\n",
        "\n",
        "parser = SimpleFileNodeParser()\n",
        "md_nodes = parser.get_nodes_from_documents(md_docs)\n",
        "for idx,i in enumerate(md_nodes):\n",
        "    print(idx,i.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNixfaNlFCyY"
      },
      "source": [
        "### DatabaseReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv2WUQtAFCyY"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.database import DatabaseReader\n",
        "db = DatabaseReader(\n",
        "    scheme=\"postgresql\",  # Database Scheme\n",
        "    host=\"test.supabase.com\",  # Database Host\n",
        "    port=\"8000\",  # Database Port\n",
        "    user=\"test.postgres\",  # Database User\n",
        "    password=\"your_password\",  # Database Password\n",
        "    dbname=\"postgres\",  # Database Name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klgNf0rxFCyZ",
        "outputId": "f529924e-ce32-47c6-d94b-e89f81410825"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id_='08d7034a-e88b-4154-a104-da822e110386', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='id: 1, created_at: 2024-02-21 13:02:31.445522+00:00, first_name: Bob, last_name: Malay', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f505869a-aabd-4c4b-9231-48c7320a4774', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='id: 2, created_at: 2024-02-21 13:02:42.008095+00:00, first_name: Alice, last_name: Layla', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ff47f1ce-fd5b-47f7-84b7-0f623a5a1fae', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='id: 3, created_at: 2024-02-21 13:03:02.936948+00:00, first_name: Casie, last_name: Lu', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db.load_data('SELECT * FROM person')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEJ6XDlPE5ar"
      },
      "source": [
        "## Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QY_UCD2E5at",
        "outputId": "2ee5efb2-b0ae-4ae1-f9d6-efcf376083a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc ID: 320dba3e-6b2f-4033-adc7-4a7130304acb\n",
            "Text: Hello\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Document\n",
        "document = Document(text = \"Hello\")\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pcIDRdqE5av",
        "outputId": "84e3b7d6-1f59-4e63-f0bc-82fd8a0874fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['id_', 'embedding', 'metadata', 'excluded_embed_metadata_keys', 'excluded_llm_metadata_keys', 'relationships', 'text', 'start_char_idx', 'end_char_idx', 'text_template', 'metadata_template', 'metadata_seperator', 'class_name'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "document.to_dict().keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bLGPxi2E5aw"
      },
      "source": [
        "## MetaData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um_wC-9oE5aw"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "document_without_meta_data = Document(text = \"Hello\")\n",
        "document_with_meta_data = Document(text = \"Hello\", metadata = {\"language\": \"EN\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VXdjOunE5aw",
        "outputId": "71481933-7951-4f9e-b7c2-0cd17613d31b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n",
            "{'language': 'EN'}\n"
          ]
        }
      ],
      "source": [
        "print(document_without_meta_data.metadata)\n",
        "print(document_with_meta_data.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM metadata"
      ],
      "metadata": {
        "id": "6Y2Z1SY_tl1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIsCB3NbE5ax",
        "outputId": "c3c11ca7-8a0a-45dd-e4cd-3d1cc180d19d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "--------\n",
            "language: EN\n",
            "\n",
            "Hello\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.schema import MetadataMode\n",
        "print(document_without_meta_data.get_content(MetadataMode.LLM))\n",
        "print('--------')\n",
        "print(document_with_meta_data.get_content(MetadataMode.LLM))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding metadata"
      ],
      "metadata": {
        "id": "UAdpAIjmttNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2LHV61BE5ax",
        "outputId": "3101aaa1-7825-4e1c-fe33-9dbf5a7940c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "--------\n",
            "language: EN\n",
            "\n",
            "Hello\n"
          ]
        }
      ],
      "source": [
        "print(document_without_meta_data.get_content(MetadataMode.EMBED))\n",
        "print('--------')\n",
        "print(document_with_meta_data.get_content(MetadataMode.EMBED))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excluded metadata keys"
      ],
      "metadata": {
        "id": "1W96vRdYtcma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "049Y9ewbE5ay"
      },
      "outputs": [],
      "source": [
        "document_with_meta_data.excluded_llm_metadata_keys = [\"language\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T1fe1N5E5ay",
        "outputId": "141f050a-5bbf-40dd-82c1-10f00e76e36e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "--------\n",
            "language: EN\n",
            "\n",
            "Hello\n"
          ]
        }
      ],
      "source": [
        "print(document_with_meta_data.get_content(MetadataMode.LLM))\n",
        "print('--------')\n",
        "print(document_with_meta_data.get_content(MetadataMode.EMBED))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI2X2gynE5az",
        "outputId": "cc90ed6f-6c67-42af-f477-24b388839271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLM sees this: \n",
            " Metadata: category=>finance, author=>LlamaIndex\n",
            "-----\n",
            "Content: This is a super-customized document\n",
            "The Embedding model sees this: \n",
            " Metadata: file_name=>super_secret_document.txt, category=>finance, author=>LlamaIndex\n",
            "-----\n",
            "Content: This is a super-customized document\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Document\n",
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "document = Document(\n",
        "    text=\"This is a super-customized document\",\n",
        "    metadata={\n",
        "        \"file_name\": \"super_secret_document.txt\",\n",
        "        \"category\": \"finance\",\n",
        "        \"author\": \"LlamaIndex\",\n",
        "    },\n",
        "    excluded_llm_metadata_keys=[\"file_name\"],\n",
        "    metadata_seperator=\", \",\n",
        "    metadata_template=\"{key}=>{value}\",\n",
        "    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"The LLM sees this: \\n\",\n",
        "    document.get_content(metadata_mode=MetadataMode.LLM),\n",
        ")\n",
        "print(\n",
        "    \"The Embedding model sees this: \\n\",\n",
        "    document.get_content(metadata_mode=MetadataMode.EMBED),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edai5XUOE99j"
      },
      "source": [
        "## Transformation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp"
      ],
      "metadata": {
        "id": "1NLigSN_y1t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpM_-WJtE99n"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"II FinOps ?\n",
        "A FinOps (cloud Financial Operations)\n",
        "FinOps Foundation เป็น Foundation ที่ได้รับการสนับสนุนจาก The Linux Foundation ในการช่วยสร้าง Foundation สำหรับการส่งเสริมและผลักดันการใช้งาน Cloud computing อย่างมีคุณค่า\n",
        "\n",
        "B History\n",
        "FinOps Foundation ก่อตั้งขึ้นมาตอน 2019 เกิดขึ้นหลังจากการมีอยู่ของ Cloud computing ประมาณ 10 ปี (AWS establish in 2006) ซึ่งถือว่าเป็น Foundation ที่มีอายุน้อยมาก (เทียบกับปี 2023) ฉะนั้นแล้วในส่วนของ Framework และ Best practice มีการอัพเดทอย่างรวดเร็วและเพิ่มขึ้นอย่างต่อเนื่อง เนื่องจาก use case ส่วนที่ยังไม่ได้ค้นพบยังมีอยู่มหาศาล\n",
        "III Cloud in real life\n",
        "การใช้งาน Cloud นั้นเริ่มต้นนั้นจะดูเข้าใจง่าย แต่ความจริงแล้ววิธีคิดเงินของ Cloud นั้นมีความซับซ้อนและความเปลี่ยนแปลงสูง ซึ่งยังมีสิ่งที่ไม่เกี่ยวข้องกับวิธีคิดเงิน แต่เกี่ยวข้องกับความรับผิดชอบบางประการที่ถูกเปลี่ยนแปลงมาโดยไม่มีใครสังเกตุ\n",
        "A Spending decision\n",
        "ความรับผิดชอบด้านการจัดซื้อถูกย้ายมาให้กับทีม Engineer โดยตั้งใจหรือไม่ตั้งใจ ก็เป็นไปได้ทั้งสองกรณี โดยปกติแล้วการตัดสินใจในรายจ่ายเราจะมอบให้กับทีม Financial หรือทีม Procurement แต่เมื่อเป็น Cloud environment เรียบร้อยแล้ว คนที่เข้าใจ Cloud มากที่สุดกลับเป็นทีม Engineer ที่ไม่มีหน้าที่ในการรับผิดชอบการใช้จ่ายของบริษัท หรือเมื่อ Spending decision ยังอยู่กับทีม Financial คนที่ต้อง Commit รายจ่ายของ Cloud computing ก็ยังคงเป็นทีม Engineer\n",
        "\n",
        "ถ้าไม่แก้ไขปัญหาจากเหตุการณ์ดังกล่าวจะเกิดเหตุการณ์ Engineer Commit รายจ่ายให้สูงเกินความจำเป็น หรือ การใช้งานบางอย่าง (Serverless) ส่งผลให้เกิดรายจ่ายที่ผิดปกติในเดือนนั้น ๆ โดยไม่ทราบสาเหตุ\n",
        "\n",
        "วิธีแก้ไขของหลายบริษัทคือการให้ Engineer ลดรายจ่ายให้อยู่ในเพดานที่เหมาะสม\n",
        "\n",
        "อ่านดูแล้วอาจเป็นวิธีการแก้ปัญหาที่ตรงจุด แต่วิธีดังกล่าวกลับเหมาะสมกับบริบทดั้งเดิม ไม่สมเหตุสมผลกับบริบทปัจจุบัน เพราะการกระจายอำนาจตัดสินใจให้กับทีมอื่นได้นั้นถือเป็นความสามารถที่ไม่เคยเกิดขึ้นมาก่อนและยังสอดคล้องกับการทำงานยุคใหม่ที่ใช้ Framework อย่างเช่น Agile ที่ต้องการให้เกิดความรวดเร็วในการตัดสินใจ\n",
        "\n",
        "B Variable spending\n",
        "การใช้งานบางบริษัทของผู้ให้บริการ Cloud computing ไม่ได้คิดค่าบริการตามจำนวนชั่วโมงที่ใช้งานอย่างที่เราเข้าใจ แต่การคิดค่าบริการอาจคิดจากจำนวนการเรียกใช้งานหรือจำนวนของ bandwidth เป็นต้น\n",
        "\n",
        "ลักษณะการคิดค่าบริการดังกล่าวมีข้อดีคือเป็นการคิดค่าใช้จ่ายแบบ Variable cost เราสามารถนำไปใช้ในการคำนวณหา Cost Of Goods Sold (COGS) ได้ทันที\n",
        "\n",
        "ส่วนข้อเสียคือถ้าทีม Financial ไม่เข้าใจที่มาของต้นทุนดังกล่าวแล้วไม่ได้ลงไว้ใน Variable cost หรือ COGS จะเกิดคำถามว่าทำไมค่าใช้จ่าย Cloud computing ในแต่ละเดือนนั้น ไม่เท่ากันและไม่สามารถพิจารณาค่าใช้จ่าย Cloud computing ล่วงหน้า\n",
        "\n",
        "วิธีแก้ไขของหลายบริษัทคือไม่ใช้งาน Cloud computing ในส่วน Variable cost หรือไม่ต้องใช้งาน Cloud ไปเลย\n",
        "\n",
        "C Scalable resource\n",
        "การปรับจำนวนทรัพยากรได้ตลอดเวลานั้นสามารถส่งเสริมให้เกิดนวัตกรรมได้ตลอดเวลา เนื่องจากทีม Engineer สามารถเข้าถึงทรัพยากรได้อย่างรวดเร็วและเข้าถึงบางทรัพยากรที่ไม่เคยมีมาก่อน เช่น การเข้าถึง High performance computer, การเข้าถึง Computer cluster เป็นต้น สิ่งเหล่านี้จะทำให้ทีม Engineer สามารถทดลองนวัตกรรมใหม่ ๆ หรือสำรวจความเป็นไปได้ใหม่ ๆ โดยปราศจากข้อจำกัดด้านการจัดซื้อ\n",
        "\n",
        "แต่ข้อเสียที่เกิดขึ้นเมื่อทีม Engineer สามารถเข้าถึงทรัพยากรเหล่านี้ได้โดยปราศจากการอนุมัติจากทีม Finance บางครั้งทีม Engineer อาจจะลืมหรือละเลยการใช้งานทรัพยากรเหล่านี้โดยเปิดทิ้งไว้ตลอดเวลา แต่ใช้งานจริงแค่เพียง 1% ของเวลาที่คิดค่าใช้จ่าย  ทำให้เกิดค่าใช้จ่ายส่วนเกินที่ไม่ควรเกิดขึ้น\n",
        "\n",
        "ดังนั้นเพื่อเป็นการป้องกันปัญหาดังกล่าวจำเป็นต้องให้เกิดการตรวจสอบการใช้งานได้ตลอดเวลา เพื่อติดตามและป้องกันไม่ให้เกิดปัญหาดังกล่าว\n",
        "\n",
        "VII  FinOps framework ถูกสร้างขึ้นมาโดยมีหัวข้อหลักทั้งหมด 5 หัวข้อ ดังนี้\n",
        "Maturity\n",
        "Principles\n",
        "Personas\n",
        "Phases\n",
        "Domains\n",
        "โดยแต่ละหัวข้อจะอธิบายถึงสถานะ, เครื่องมือ, กิจกรรม, ความเข้ากันได้ หรือ สิ่งขับเคลื่อน ซึ่งแต่ละหัวข้อสามารถแยกพิจารณาเมื่อไหร่ก็ได้ ไม่จำเป็นต้องเรียงลำดับและในแต่ละบริษัทไม่จำที่จะต้องให้น้ำหนักกับแต่ละหัวข้อเหมือนกันทุกบริษัท ขึ้นอยู่กับบริบทของบริษัทนั้น ๆ\n",
        "\n",
        "เราขอเริ่มอธิบายจากหัวข้อแรก\n",
        "\n",
        "A Maturity\n",
        "Maturity เราอาจจะเรียกได้ว่าเป็นประสบการณ์ของทีม แต่ในที่นี้ขอเรียกว่าความเข้ากันได้ เนื่องจากบางบริษัททีมมีความเข้ากันได้อยู่แล้ว เคยทำงานร่วมกันด้วยความเข้าใจดีเยี่ยม ความเข้ากันได้ของทีมนั้น ๆ ก็จะสูงตาม\n",
        "\n",
        "ในบริบทของ FinOps เราแบ่ง Maturity ออกเป็น 3 ช่วง ได้แก่\n",
        "\n",
        "Crawl\n",
        "ช่วงแรกเริ่มของการนำ FinOps เข้ามาประยุกต์เข้ากับบริษัทของตัวเอง ซึ่งเราสามารถประเมินได้ว่าบริษัทที่อยู่ในช่วงดังกล่าวได้ว่า กระบวนการทำงาน, กระบวนการรายงานผล ยังอยู่ในรูปแบบของ Manual หรือไม่ได้ใช้ระบบอัตโนมัติเข้ามาช่วย รวมถึงการทำงานบางส่วน เช่น Forecast ค่าใช้จ่าย ยังคงต้องคำนวนใหม่ทุกครั้ง ยังไม่ได้สร้างสูตรหรือโมเดลในการช่วยคำนวน ต้องให้ทีม Finance ประเมินทุก ๆ คร้ัง\n",
        "\n",
        "Walk\n",
        "บางบริษัทอาจจะเริ่มจากช่วงนี้เลยก็เป็นไปได้ สำหรับช่วง walk นั้นจะเรียกได้ว่าเป็นช่วงของ Semi-Automate process เนื่องจากทีมมีบุคคลที่มีความสามารถในการสร้าง Automate process ด้วยตัวเอง ไม่ว่าจะเป็นทีม Engineer, ทีม Finance หรือทีม Finance ทำให้งานบางประเภทนั้นมีความสามารถในการทำงานอย่างอัตโนมัติ Automate process ที่สร้างขึ้นมานั้นสามารถให้คนที่ไม่มีความเชี่ยวชาญที่เกี่ยวข้องสามารถใช้งานได้โดยไม่มีผู้เชี่ยวชาญ\n",
        "\n",
        "Run\n",
        "“ทีมที่เพียบพร้อมด้วยสหวิทยาการ” นี่คือคำนิยามแบบเข้าใจง่ายของช่วงนี้ เพราะทีมที่มีความเข้ากันได้สูง สามารถสร้าง Automate process ออกมาได้ตรงประเด็น จำเป็นต้องมีความเข้าใจทั้งในส่วนที่ตัวเองเชี่ยวชาญและความเข้าใจในส่วนที่นอกเหนือจากที่ตัวเองเชี่ยวชาญ เพื่อที่จะสร้างระบบเพื่อส่งมอบข้อมูลเพื่อนำไปใช้ในการตัดสินได้ตลอดเวลา (Realtime) และช่วยให้บุคคลอื่นสามารถตัดสินใจบนข้อมูลได้อย่างทันท่วงที ไม่จำเป็นต้องปรึกษากับทีมอื่น\n",
        "\n",
        "B Principles\n",
        "Principles คือสิ่งที่ระบุถึงสิ่งที่จำเป็นต้องบรรลุเมื่อต้องการนำ FinOps เข้ามาใช้ในบริษัท ส่วนของ Principles นั้นจะมีความใกล้เคียงกับ Framework ที่เกี่ยวข้องกับ Digital transformation อย่างเช่น Agile หรือ DevOps เนื่องจากต้องการให้บริษัทไม่จำเป็นต้องปรับตัวทั้งหมดเพื่อที่จะเริ่มนำ FinOps ไปใช้งาน โดย Principles นั้นมีอยู่ทั้งหมด 6 ข้อได้แก่\n",
        "\n",
        "ทีมต้องทำงานร่วมกัน\n",
        "ทุกคนต้องเป็นเจ้าของการใช้งาน Cloud computing ร่วมกัน\n",
        "FinOps นั้นจะถูกขับเคลื่อนโดยทีมส่วนกลาง\n",
        "Report ที่เกี่ยวข้อง (Engineer, Finance, Business) ต้องเข้าถึงได้และทันท่วงที\n",
        "การตัดสินใจต้องถูกขับเคลื่อนโดยคุณค่าด้านธุรกิจผ่านการใช้งาน Cloud computing\n",
        "ใช้ความได้เปรียบด้าน Variable cost จาก Cloud computing\n",
        "C Personas\n",
        "Personas ในที่นี้จะระบุถึงแรงจูงใจ, ปัญหา, ตัวชี้วัดและสิ่งที่ได้จาก FinOps ซึ่ง Personas นั้นจะถูกแบ่งออกเป็น 5 บุคคลที่เกี่ยวข้องได้แก่\n",
        "\n",
        "1 FinOps Practitioner\n",
        "\n",
        "บุคคลผู้ซึ่งต้องการนำ FinOps เข้ามาใช้กับกระบวนการทำงานปัจจุบันของบริษัท โดยเป็นผู้ที่มีความเข้าใจใน Personas ของผู้อื่นมากที่สุด นิยามอย่างง่ายคือ ทีม Finance มองว่า (Practitioner) เป็นคนจากทีม Engineer แต่ทีม Engineer มองว่าเป็นคนจากทีม Fiannce\n",
        "\n",
        "2 Executive\n",
        "\n",
        "ผู้บริหารคืออีกหนึ่ง Personas ที่ต้องเข้ามามีส่วนเกี่ยวข้องในเงื่อนของการคาดหวังผลลัพธ์และต้องมีส่วนช่วยในการสนับสนุน (Sponsor) การเปลี่ยนแปลงดังกล่าวให้เกิดขึ้นให้ได้\n",
        "\n",
        "3 Business\n",
        "\n",
        "ทีม Business หรือ Product owner ผู้ซึ่งต้องดูแลการเติบโตและวางเป้าหมายให้ตัวผลิตภัฑณ์หรือบริการที่กำลังดูแลอยู่นั้น ส่งมอบมูลค่าแก่ลูกค้าได้อย่างครบถ้วนและตรงจุด เป็นผู้ซึ่งเข้าใจในส่วนของ User หรือ Customer ได้ดีที่สุดจากทั้ง 5 บุคคลที่เกี่ยวข้อง\n",
        "\n",
        "4 Finance\n",
        "\n",
        "ทีม Finance เป็นทีมที่ดูแลเกี่ยวกับการลงทุนในแต่ละโปรเจคและต้องคอยประเมินถึงค่าใช้จ่ายระยะยาว โดยต้องประเมินถึงความคุ้มทุนของโปรเจคที่ต้องลงทุนและประเมินว่าเมื่อถึงเวลาใดจะเป็นจุด Break event point\n",
        "\n",
        "5 Engineer\n",
        "\n",
        "บุคคลที่เป็นคนตัดสินความเป็นไปได้ของการสร้างผลิตภัฑณ์หรือบริการ ซึ่งความเป็นไปได้สำหรับผลิตภัฑณ์หรือบริการทางด้าน software นั้น ตั้งอยู่บนพื้นฐาน 3 สิ่งคือ Quality, Cost และ Time สามารถเลือกได้แค่ 2 ใน 3 เท่านั้น\n",
        "\n",
        "D Phases\n",
        "Phase กล่าวคือวงวนกระบวนการสำหรับ FinOps โดยกระบวนการดังกล่าวนั้นสามารถเริ่มจากกระบวนการใดก็ได้ ไม่จำเป็นต้องเริ่มจาก Inform แต่จำเป็นต้องทำตามลำดับเช่น เริ่มจาก Optimize จากนั้นเรียงไป Operate และ Inform ตามลำดับ ซึ่งเมื่อเราสามารถวนครบรอบได้แล้ว เราสามารถทำต่อไปได้ไม่มีที่สิ้นสุด เนื่องจากทุก ๆ ครั้งในการครบรอบสิ่งที่ทีมจะได้เพิ่มคือความเข้ากันได้ของทีม (Maturity)\n",
        "\n",
        "Phase แบ่งออกด้วยกันทั้งหมด 3 phase ได้แก่\n",
        "\n",
        "1 Inform\n",
        "\n",
        "Phase inform จะเน้นผลลัพธ์ในการสร้างการเข้าถึงข้อมูล, การจัดการข้อมูล, การวัดผล, การกำหนดรายจ่าย และการ forecast รายจ่าย ในส่วนของ Inform จำเป็นต้องสร้างตัวชี้วัดโดยอาศัยข้อมูลจากทีมอื่นเข้ามาเป็นตัวแปรร่วมด้วย\n",
        "\n",
        "2 Optimize\n",
        "\n",
        "หลาย ๆ บริษัทมักอยากจะกระโดดเข้ามาใน Phase นี้เป็นส่วนแรก เนื่องจากเห็นผลได้รวดเร็วและมองเห็นถึงผลลัพธ์ได้ง่าย ในส่วนของ FinOps Foundation นั้นให้ความหมายของ Phase นี้เอาไว้ว่า เป็นช่วงที่ต้องมองหาความเป็นไปได้ในการลดต้นทุนและใช้ทรัพยากรที่มีอยู่ให้มีประสิทธิภาพสูงที่สุด หรือเข้าใจโดยง่ายว่า การ Optimize ทุกอย่างนั้นจะเกิดขึ้นแบบทฤษฎีก่อนนำไปให้ทีมที่เกี่ยวข้องนั้นนำไปใช้งานจริง\n",
        "\n",
        "3 Operate\n",
        "\n",
        "Phase นี้มักเป็นส่วนที่ถูกมองข้ามในการจำกัดความแต่ในการปฏิบัตินั้นเกิดขึ้นโดยไม่รู้ตัว ซึ่งการ Operate จะเป็นการเริ่มติดตามถึงตัวชี้วัดที่เราวางไว้, ติดตามถึงผลกระทบในส่วนของ Quality, Cost และ Time และในส่วนของการลงมือนำแผนการจาก Optimize นำไปใช้จริง\n",
        "\n",
        "E Domain\n",
        "Domain ระบุถึงกลุ่มกิจกรรมที่ให้ผลลัพธ์ออกมาในกลุ่มผลลัพธ์เดียวกัน โดย Domain นั้นจะถูกนำไปใช้ในแต่ Phase โดย Domain นั้นจะถูกกำหนดไว้ ณ ปัจจุบัน 6 Domain ได้แก่\n",
        "\n",
        "1 Understanding cloud usage and cost\n",
        "\n",
        "สำหรับ Domain นี้คือการรวมรวบข้อมูลที่เกี่ยวข้องกับการใช้งาน Cloud computing และนำข้อมูลที่ได้มานั้ดจัดให้อยู่ในรูปแบบที่สามารถเข้าถึงได้ ซึ่งข้อมูลดังกล่าวต้องตอบสนองต่อแต่ละ Personas อย่างครบถ้วน\n",
        "\n",
        "2 Performance Tracking & Benchmarking\n",
        "\n",
        "สำหรับการวัดผลและตั้งเป้าหมายจะถูกกำหนดขึ้น โดยเริ่มจากการตั้งงบประมาณ, การกำหนดผลลัพธ์ที่คาดหวัง หรือ ใช้ข้อมูลย้อนหลังเพื่อเป็นสมมติฐานการคำนวนรายจ่ายในอนาคต การวัดผลและการสร้างตัวชี้วัดทั้งหมดจะถูกสร้างใน Domain นี้\n",
        "\n",
        "3 Real-Time Decision Making\n",
        "\n",
        "การสร้างระบบสำหรับตัดสินใจแบบ Real time จะช่วยให้ผู้ที่มีส่วนได้ส่วนเสียทั้งหมดมีประสิทธิภาพในการรับมือกับสถานการณ์ต่าง ๆ ได้ดียิ่งขึ้น ไม่ว่าจะเป็นในด้านของเวลาในการตัดสินใจ หรือ คุณภาพในการตัดสินใจซึ่งต้องไปในทิศทางเดียวกันกับบริษัท\n",
        "\n",
        "4 Cloud Rate Optimization\n",
        "\n",
        "ภายใน Domain นี้จะกล่าวถึงการทำความเข้าใจเกี่ยวกับรูปแบบการคิดค่าบริการของผู้ให้บริการ Cloud computing และทำการปรับปรุงรายจ่าย Cloud computing ของบริษัทให้สอดคล้องกับรูปแบบการคิดค่าบริการของผู้ให้บริการ โดยคำนึงถึงข้อจำกัดที่แปรผันตามรูปแบบการคิดค่าบริการที่ต่างไป\n",
        "\n",
        "5 Cloud Usage Optimization\n",
        "\n",
        "Domain นี้ให้ความสำคัญถึงการใช้ทรัพยากรที่มีอยู่ให้คุ้มค่าที่สุด โดยการระบุและลงมือเปลี่ยนแปลงงานบางประเภทให้ประมวลผลให้ช่วงเวลาที่ Cloud computing นั้นไม่ได้ใช้งาน และรวมถึงการเปลี่ยนแปลงทรัพยากรที่มีอยู่ในเหมาะสมกับประเภทของงาน\n",
        "\n",
        "6 Organizational Alignment\n",
        "สำหรับ Domain นี้จะกล่าวถึงการที่บริษัทนั้นมีทิศทางในการใช้งาน Cloud computing ในทิศทางเดียวกันผ่าน Principles ของ FinOps ร่วมกัน เพื่อที่จะกำหนดวัฒนธรรมและนโยบายสำหรับบริษัทโดยไม่ขัดแย้งต่อวัฒนธรรมเดิมที่มีอยู่ของบริษัท\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pall4NnPE99r",
        "outputId": "4699e10c-520a-4ce2-e050-b61a181da77d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "8dd4f7aa998a400382ccc270a59268b6",
            "a8bf614f7bb5494dbf28c2d4e6df358d",
            "18a3c33192244cf3b57fc6727ae342d1",
            "bc401d29b43e409a9d7852a497c7386d",
            "3b8d49cb9ee04f2d81865644a4ff4be9",
            "9429e892d8f24f79a93babb0d84f2c3c",
            "9778af3baa724840ba54ad659a8dc04d",
            "cc7c1033ddb749f28ee9e72156d31504",
            "668a14539c36465187cdb0353926526a",
            "dead8c7ac7f64dfc8650acff918abb6d",
            "033d0a9f727544b1a543662176c40233",
            "5382bc139f82478aafe11ac8665348c7",
            "e559b5688b574f538f40f1f28a0ee49c",
            "e2b27a96df54478d8d39a6a2b255dfed",
            "d2b45bd094ec48f5b5fa38403016518b",
            "fdf55aaf62884b18a11b6614a2c3ff14",
            "cb1979a937c040bdbbd8c477359c2eca",
            "809722233d234402a41f1c93e767b1ec",
            "9a17799034544cb0939a3c87e6600ef8",
            "4b9a08a679fa455c962b689b2963e984",
            "a127f67bd108480cb8bf4f4f07bce943",
            "6480c7972e314f9e8333943567c7cb69",
            "a0253a6e90ef4b758f9fe13f8d65e7ce",
            "5c21cd5cc45a418688bea102c7347d95",
            "5163bbcb79944b36b12ceed50455c9fe",
            "0b524fd71da44180bd433ed314bb56ce",
            "08668df771bc46b088fe34863df16d23",
            "aff0d510ef494db7aead5bcc35f01167",
            "54153b7bb3f94906b77d259440898243",
            "30fcba22562f42c2b1b4d59f86290111",
            "76f8199eb1d0437d9c32a78c7b7d0604",
            "2565453171c84abe9c030f8599fd31ee",
            "f64e2ee94d634f128cdd29583e1a42b5",
            "44ae49ccd4344e72ab62b38c0f5c31cc",
            "83243e18d2144458ae2a813a91b2b593",
            "c389921042844af58430a79d53b4bdb8",
            "22c76a03fd3e48ca86e511a3fcb0e91f",
            "c714206caa0c4be3b0b1a34194c4fbc3",
            "7731e095284c4a539ed4bbb9f6335f4d",
            "92b98cc497064128b06d408e405f7285",
            "23568a724c3a419d9611337d3ed1f14a",
            "b7a5dea284324bbdb377ef6634cd596c",
            "34918ebb4aaf4e13aba54ddb718c1693",
            "8299e6b01a5c4909ad0136e0f9bf7c7b"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dd4f7aa998a400382ccc270a59268b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/780k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5382bc139f82478aafe11ac8665348c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.66M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0253a6e90ef4b758f9fe13f8d65e7ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44ae49ccd4344e72ab62b38c0f5c31cc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from pythainlp.tokenize import sent_tokenize\n",
        "import tiktoken\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Callable, List\n",
        "\n",
        "def thai_sent_tokenize() -> Callable[[str], List[str]]:\n",
        "    def split(text: str) -> List[str]:\n",
        "        return sent_tokenize(text)\n",
        "    return split\n",
        "\n",
        "spliter = SentenceSplitter(\n",
        "    separator=\" \",\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=256,\n",
        "    paragraph_separator='\\n',\n",
        "    # tokenizer=tiktoken.encoding_for_model('gpt-3.5-turbo').encode,\n",
        "    tokenizer=AutoTokenizer.from_pretrained(\"SeaLLMs/SeaLLM-7B-v2\").encode,\n",
        "    # chunking_tokenizer_fn=thai_sent_tokenize(),\n",
        "    include_metadata=True,\n",
        "    include_prev_next_rel=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtOHr4ErE99s",
        "outputId": "e9d58af4-9943-4473-e65f-e3d0915aedb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12672 5\n",
            "II FinOps ?\n",
            "A FinOps (cloud Financial Operations)\n",
            "FinOps Foundation เป็น Foundation ที่ได้รับการสนับสนุนจาก The Linux Foundation ในการช่วยสร้าง Foundation สำหรับการส่งเสริมและผลักดันการใช้งาน Cloud computing อย่างมีคุณค่า\n",
            "\n",
            "B History\n",
            "FinOps Foundation ก่อตั้งขึ้นมาตอน 2019 เกิดขึ้นหลังจากการมีอยู่ของ Cloud computing ประมาณ 10 ปี (AWS establish in 2006) ซึ่งถือว่าเป็น Foundation ที่มีอายุน้อยมาก (เทียบกับปี 2023) ฉะนั้นแล้วในส่วนของ Framework และ Best practice มีการอัพเดทอย่างรวดเร็วและเพิ่มขึ้นอย่างต่อเนื่อง เนื่องจาก use case ส่วนที่ยังไม่ได้ค้นพบยังมีอยู่มหาศาล\n",
            "III Cloud in real life\n",
            "การใช้งาน Cloud นั้นเริ่มต้นนั้นจะดูเข้าใจง่าย แต่ความจริงแล้ววิธีคิดเงินของ Cloud นั้นมีความซับซ้อนและความเปลี่ยนแปลงสูง ซึ่งยังมีสิ่งที่ไม่เกี่ยวข้องกับวิธีคิดเงิน แต่เกี่ยวข้องกับความรับผิดชอบบางประการที่ถูกเปลี่ยนแปลงมาโดยไม่มีใครสังเกตุ\n",
            "A Spending decision\n",
            "ความรับผิดชอบด้านการจัดซื้อถูกย้ายมาให้กับทีม Engineer โดยตั้งใจหรือไม่ตั้งใจ ก็เป็นไปได้ทั้งสองกรณี โดยปกติแล้วการตัดสินใจในรายจ่ายเราจะมอบให้กับทีม Financial หรือทีม Procurement แต่เมื่อเป็น Cloud environment เรียบร้อยแล้ว คนที่เข้าใจ Cloud มากที่สุดกลับเป็นทีม Engineer ที่ไม่มีหน้าที่ในการรับผิดชอบการใช้จ่ายของบริษัท หรือเมื่อ Spending decision ยังอยู่กับทีม Financial คนที่ต้อง Commit รายจ่ายของ Cloud computing ก็ยังคงเป็นทีม Engineer\n",
            "\n",
            "ถ้าไม่แก้ไขปัญหาจากเหตุการณ์ดังกล่าวจะเกิดเหตุการณ์ Engineer Commit รายจ่ายให้สูงเกินความจำเป็น หรือ การใช้งานบางอย่าง (Serverless) ส่งผลให้เกิดรายจ่ายที่ผิดปกติในเดือนนั้น ๆ โดยไม่ทราบสาเหตุ\n",
            "\n",
            "วิธีแก้ไขของหลายบริษัทคือการให้ Engineer ลดรายจ่ายให้อยู่ในเพดานที่เหมาะสม\n",
            "\n",
            "อ่านดูแล้วอาจเป็นวิธีการแก้ปัญหาที่ตรงจุด แต่วิธีดังกล่าวกลับเหมาะสมกับบริบทดั้งเดิม ไม่สมเหตุสมผลกับบริบทปัจจุบัน เพราะการกระจายอำนาจตัดสินใจให้กับทีมอื่นได้นั้นถือเป็นความสามารถที่ไม่เคยเกิดขึ้นมาก่อนและยังสอดคล้องกับการทำงานยุคใหม่ที่ใช้ Framework อย่างเช่น Agile ที่ต้องการให้เกิดความรวดเร็วในการตัดสินใจ\n",
            "\n",
            "B Variable spending\n",
            "การใช้งานบางบริษัทของผู้ให้บริการ Cloud computing ไม่ได้คิดค่าบริการตามจำนวนชั่วโมงที่ใช้งานอย่างที่เราเข้าใจ แต่การคิดค่าบริการอาจคิดจากจำนวนการเรียกใช้งานหรือจำนวนของ bandwidth เป็นต้น\n",
            "\n",
            "ลักษณะการคิดค่าบริการดังกล่าวมีข้อดีคือเป็นการคิดค่าใช้จ่ายแบบ Variable cost เราสามารถนำไปใช้ในการคำนวณหา Cost Of Goods Sold (COGS) ได้ทันที\n",
            "\n",
            "ส่วนข้อเสียคือถ้าทีม Financial ไม่เข้าใจที่มาของต้นทุนดังกล่าวแล้วไม่ได้ลงไว้ใน Variable cost หรือ COGS จะเกิดคำถามว่าทำไมค่าใช้จ่าย Cloud computing ในแต่ละเดือนนั้น ไม่เท่ากันและไม่สามารถพิจารณาค่าใช้จ่าย Cloud computing ล่วงหน้า\n",
            "\n",
            "วิธีแก้ไขของหลายบริษัทคือไม่ใช้งาน Cloud computing ในส่วน Variable cost หรือไม่ต้องใช้งาน Cloud ไปเลย\n",
            "\n",
            "C Scalable resource\n",
            "การปรับจำนวนทรัพยากรได้ตลอดเวลานั้นสามารถส่งเสริมให้เกิดนวัตกรรมได้ตลอดเวลา เนื่องจากทีม Engineer สามารถเข้าถึงทรัพยากรได้อย่างรวดเร็วและเข้าถึงบางทรัพยากรที่ไม่เคยมีมาก่อน เช่น การเข้าถึง High performance computer, การเข้าถึง Computer cluster เป็นต้น สิ่งเหล่านี้จะทำให้ทีม Engineer สามารถทดลองนวัตกรรมใหม่ ๆ หรือสำรวจความเป็นไปได้ใหม่ ๆ โดยปราศจากข้อจำกัดด้านการจัดซื้อ\n",
            "---\n",
            "ส่วนข้อเสียคือถ้าทีม Financial ไม่เข้าใจที่มาของต้นทุนดังกล่าวแล้วไม่ได้ลงไว้ใน Variable cost หรือ COGS จะเกิดคำถามว่าทำไมค่าใช้จ่าย Cloud computing ในแต่ละเดือนนั้น ไม่เท่ากันและไม่สามารถพิจารณาค่าใช้จ่าย Cloud computing ล่วงหน้า\n",
            "\n",
            "วิธีแก้ไขของหลายบริษัทคือไม่ใช้งาน Cloud computing ในส่วน Variable cost หรือไม่ต้องใช้งาน Cloud ไปเลย\n",
            "\n",
            "C Scalable resource\n",
            "การปรับจำนวนทรัพยากรได้ตลอดเวลานั้นสามารถส่งเสริมให้เกิดนวัตกรรมได้ตลอดเวลา เนื่องจากทีม Engineer สามารถเข้าถึงทรัพยากรได้อย่างรวดเร็วและเข้าถึงบางทรัพยากรที่ไม่เคยมีมาก่อน เช่น การเข้าถึง High performance computer, การเข้าถึง Computer cluster เป็นต้น สิ่งเหล่านี้จะทำให้ทีม Engineer สามารถทดลองนวัตกรรมใหม่ ๆ หรือสำรวจความเป็นไปได้ใหม่ ๆ โดยปราศจากข้อจำกัดด้านการจัดซื้อ\n",
            "\n",
            "แต่ข้อเสียที่เกิดขึ้นเมื่อทีม Engineer สามารถเข้าถึงทรัพยากรเหล่านี้ได้โดยปราศจากการอนุมัติจากทีม Finance บางครั้งทีม Engineer อาจจะลืมหรือละเลยการใช้งานทรัพยากรเหล่านี้โดยเปิดทิ้งไว้ตลอดเวลา แต่ใช้งานจริงแค่เพียง 1% ของเวลาที่คิดค่าใช้จ่าย  ทำให้เกิดค่าใช้จ่ายส่วนเกินที่ไม่ควรเกิดขึ้น\n",
            "\n",
            "ดังนั้นเพื่อเป็นการป้องกันปัญหาดังกล่าวจำเป็นต้องให้เกิดการตรวจสอบการใช้งานได้ตลอดเวลา เพื่อติดตามและป้องกันไม่ให้เกิดปัญหาดังกล่าว\n",
            "\n",
            "VII  FinOps framework ถูกสร้างขึ้นมาโดยมีหัวข้อหลักทั้งหมด 5 หัวข้อ ดังนี้\n",
            "Maturity\n",
            "Principles\n",
            "Personas\n",
            "Phases\n",
            "Domains\n",
            "โดยแต่ละหัวข้อจะอธิบายถึงสถานะ, เครื่องมือ, กิจกรรม, ความเข้ากันได้ หรือ สิ่งขับเคลื่อน ซึ่งแต่ละหัวข้อสามารถแยกพิจารณาเมื่อไหร่ก็ได้ ไม่จำเป็นต้องเรียงลำดับและในแต่ละบริษัทไม่จำที่จะต้องให้น้ำหนักกับแต่ละหัวข้อเหมือนกันทุกบริษัท ขึ้นอยู่กับบริบทของบริษัทนั้น ๆ\n",
            "\n",
            "เราขอเริ่มอธิบายจากหัวข้อแรก\n",
            "\n",
            "A Maturity\n",
            "Maturity เราอาจจะเรียกได้ว่าเป็นประสบการณ์ของทีม แต่ในที่นี้ขอเรียกว่าความเข้ากันได้ เนื่องจากบางบริษัททีมมีความเข้ากันได้อยู่แล้ว เคยทำงานร่วมกันด้วยความเข้าใจดีเยี่ยม ความเข้ากันได้ของทีมนั้น ๆ ก็จะสูงตาม\n",
            "\n",
            "ในบริบทของ FinOps เราแบ่ง Maturity ออกเป็น 3 ช่วง ได้แก่\n",
            "\n",
            "Crawl\n",
            "ช่วงแรกเริ่มของการนำ FinOps เข้ามาประยุกต์เข้ากับบริษัทของตัวเอง ซึ่งเราสามารถประเมินได้ว่าบริษัทที่อยู่ในช่วงดังกล่าวได้ว่า กระบวนการทำงาน, กระบวนการรายงานผล ยังอยู่ในรูปแบบของ Manual หรือไม่ได้ใช้ระบบอัตโนมัติเข้ามาช่วย รวมถึงการทำงานบางส่วน เช่น Forecast ค่าใช้จ่าย ยังคงต้องคำนวนใหม่ทุกครั้ง ยังไม่ได้สร้างสูตรหรือโมเดลในการช่วยคำนวน ต้องให้ทีม Finance ประเมินทุก ๆ คร้ัง\n",
            "\n",
            "Walk\n",
            "บางบริษัทอาจจะเริ่มจากช่วงนี้เลยก็เป็นไปได้ สำหรับช่วง walk นั้นจะเรียกได้ว่าเป็นช่วงของ Semi-Automate process เนื่องจากทีมมีบุคคลที่มีความสามารถในการสร้าง Automate process ด้วยตัวเอง ไม่ว่าจะเป็นทีม Engineer, ทีม Finance หรือทีม Finance ทำให้งานบางประเภทนั้นมีความสามารถในการทำงานอย่างอัตโนมัติ Automate process ที่สร้างขึ้นมานั้นสามารถให้คนที่ไม่มีความเชี่ยวชาญที่เกี่ยวข้องสามารถใช้งานได้โดยไม่มีผู้เชี่ยวชาญ\n",
            "\n",
            "Run\n"
          ]
        }
      ],
      "source": [
        "nodes = spliter.split_text(text)\n",
        "total = 0\n",
        "for i in nodes :\n",
        "    total += len(i)\n",
        "print(total,len(nodes))\n",
        "print(nodes[0])\n",
        "print('---')\n",
        "print(nodes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IM2m9rmE99t",
        "outputId": "f86fd209-4e01-48ff-c1c2-2dbaac120bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13237 23\n",
            "II FinOps ?\n",
            "A FinOps (cloud Financial Operations)\n",
            "FinOps Foundation เป็น Foundation ที่ได้รับการสนับสนุนจาก The Linux Foundation ในการช่วยสร้าง Foundation สำหรับการส่งเสริมและผลักดันการใช้งาน Cloud computing อย่างมีคุณค่า\n",
            "\n",
            "B History\n",
            "FinOps Foundation ก่อตั้งขึ้นมาตอน 2019 เกิดขึ้นหลังจากการมีอยู่ของ Cloud computing ประมาณ 10 ปี (AWS establish in 2006) ซึ่งถือว่าเป็น Foundation ที่มีอายุน้อยมาก (เทียบกับปี 2023) ฉะนั้นแล้วในส่วนของ Framework และ Best practice มีการอัพเดทอย่างรวดเร็วและเพิ่มขึ้นอย่างต่อเนื่อง เนื่องจาก use case\n",
            "---\n",
            "2023) ฉะนั้นแล้วในส่วนของ Framework และ Best practice มีการอัพเดทอย่างรวดเร็วและเพิ่มขึ้นอย่างต่อเนื่อง เนื่องจาก use case ส่วนที่ยังไม่ได้ค้นพบยังมีอยู่มหาศาล\n",
            "III Cloud in real life\n",
            "การใช้งาน Cloud นั้นเริ่มต้นนั้นจะดูเข้าใจง่าย แต่ความจริงแล้ววิธีคิดเงินของ Cloud นั้นมีความซับซ้อนและความเปลี่ยนแปลงสูง ซึ่งยังมีสิ่งที่ไม่เกี่ยวข้องกับวิธีคิดเงิน แต่เกี่ยวข้องกับความรับผิดชอบบางประการที่ถูกเปลี่ยนแปลงมาโดยไม่มีใครสังเกตุ\n",
            "A Spending decision\n",
            "ความรับผิดชอบด้านการจัดซื้อถูกย้ายมาให้กับทีม Engineer โดยตั้งใจหรือไม่ตั้งใจ ก็เป็นไปได้ทั้งสองกรณี โดยปกติแล้วการตัดสินใจในรายจ่ายเราจะมอบให้กับทีม Financial หรือทีม Procurement แต่เมื่อเป็น Cloud\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from pythainlp.tokenize import sent_tokenize\n",
        "import tiktoken\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Callable, List\n",
        "\n",
        "token_spliter = TokenTextSplitter(\n",
        "    # tokenizer=tiktoken.encoding_for_model('gpt-3.5-turbo').encode,\n",
        "    tokenizer=AutoTokenizer.from_pretrained(\"SeaLLMs/SeaLLM-7B-v2\").encode,\n",
        "    separator=\" \",\n",
        "    chunk_size=256,\n",
        "    chunk_overlap=64,\n",
        "    include_metadata=True,\n",
        "    include_prev_next_rel=True,\n",
        ")\n",
        "\n",
        "nodes = token_spliter.split_text(text)\n",
        "total = 0\n",
        "for i in nodes :\n",
        "    total += len(i)\n",
        "print(total,len(nodes))\n",
        "print(nodes[0])\n",
        "print('---')\n",
        "print(nodes[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkruJPbQFI44"
      },
      "source": [
        "# Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKUo3i6IFI45",
        "outputId": "cba6d4a3-0509-44ec-e813-17ecc15e8416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "e3b85b19af16463bb0ebc3aefe3274b9",
            "e222a6d4ae674213adcbd949675a2918",
            "e19ef9357a3a4790a944b225ee7be804",
            "0ec153c366434a1fb489ae56ac020429",
            "95c0f7981aa64723a9378cf013ff40db",
            "6733fb49af9b4df287333bb5a215db10",
            "bb6185e0bcb545a3a860b55ce9c058dd",
            "e693d094b0c94034b8ae312b9131d93d",
            "a5365edf9b984ae0b159821909f65ea2",
            "f77234ac1a2c4b1eac82f5dfb3ed9da2",
            "5e823a5929694d24a200e45ee9fe1549",
            "e4ed0bd42e6c4237aa554ff891e8a3eb",
            "acff5ccd7ede4e0dac9d8991a98124c7",
            "67e1944b56f94243a81854d7fb70ff1b",
            "bf195af1747b4e1eb557cc9a2092fac7",
            "b849c09ba563408db41c70debed13108",
            "363f0e1095ae42d1a4b659e9d7a08f7c",
            "db8ca3bc4f284a749fb6790d5c0fd620",
            "91284415f258459896810c076c225a00",
            "05ca068f365149c58d02b49f70c3b856",
            "52e6430d264346469a48a2b78b2d392e",
            "ab8b5575d4724e7d8583c735cc8693e6",
            "6414efe767ff46bba640fa2733881769",
            "89e02f6dee2d4ad9aa900c49419fcea1",
            "831f9ed054334a6ebcaa687ce466ad3a",
            "040396640a7a435292c439b1d483eef5",
            "e20d7ef7da21425a80a70e35536879b4",
            "014ee68ffb5f46d6afe080041589adea",
            "4397e90b5328499da28c3cb933b2125b",
            "05df7a57d60d4a1697105c0c01d093ea",
            "9ed5d926bb18466dae2f3d245e3fae5b",
            "4df79ff379eb481c822a5a3e82a1bfa5",
            "fe8562edf8d64222b3d0f231b52a74c2",
            "c290bc2da7ac46dc9801f0a817a3410e",
            "e678e36f4bdb4977a2e49bc55a52c346",
            "20cadb3527fb41f0aa45931ce8ef608e",
            "d616011a7ad442f1af807ca39620c4e2",
            "ce62e9da75794f12be7e40ba08043645",
            "950b9c86cc3841df9081061b91d14eb4",
            "4d47bd75aa944ead91624d7da230a42b",
            "d70a1a9c114e4893bed52ba0f769681a",
            "adbb45b87c084fb092dccc14f26c192d",
            "431939de8d1245f6a757a5b6b69d0bf8",
            "530e88a8eae04922b1f0882afd982733",
            "680082e54ca3455f8fe06a5d476868c7",
            "a29fdf4b6c9d404980b02fc874a01c15",
            "069420731e6a423ca2b369b5b7551659",
            "82b8b358f5ac4780b33008d0d90a603d",
            "d551f324c95f40d4b04c9275224492c9",
            "b035c36868374f8097dada270cf94b46",
            "d1ae6482b93943e7b224edcc0e3cb464",
            "7c9eb7377d774aaab6af84cda5b2ed7e",
            "daf52fb3f5b149c6b21b03ba77d8a05e",
            "520c6691aa954859bb425cac7512d703",
            "d5b874275858455f9e2f3d277765ccbe",
            "e92f06601e1648378a9c54fce50a1545",
            "8093db9b41d1444a8710475bc40263fd",
            "cd540d5b69e647d08d1854fed30dafba",
            "8835a727579e404e97d85a40430fb71c",
            "740ee01875a54392b70180d3eaa8b9c9",
            "f1606eeb25b147fcbf6bd1543fcec61b",
            "68d5968b5e134406bbd0e6dbe205768a",
            "44030f4e93aa48fb9e1296b4cdc135be",
            "7ba23484279b4bef860bf9d84f071092",
            "b14c791dae704aa1ba2baa7f9a2723e5",
            "e3d626f9f7d94cdb9a69b53ef3de39fd"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3b85b19af16463bb0ebc3aefe3274b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4ed0bd42e6c4237aa554ff891e8a3eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6414efe767ff46bba640fa2733881769"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c290bc2da7ac46dc9801f0a817a3410e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "680082e54ca3455f8fe06a5d476868c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e92f06601e1648378a9c54fce50a1545"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.readers.file import FlatReader\n",
        "from pathlib import Path\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\")\n",
        "documents = FlatReader().load_data(Path(\"./MockupData/data.md\"))\n",
        "index = VectorStoreIndex.from_documents(documents,embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(index)"
      ],
      "metadata": {
        "id": "76fZywE2dc8V",
        "outputId": "497c1267-6be0-4f38-a5a3-3e661116265f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x7b9f53f77f40>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyURAzD5FNXr"
      },
      "source": [
        "# Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "in memory"
      ],
      "metadata": {
        "id": "HBvuX4ty9k-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.readers.file import FlatReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from pathlib import Path\n",
        "\n",
        "sentence_spilit = SentenceSplitter(\n",
        "    chunk_size=1024\n",
        ")\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\",device='cpu')\n",
        "md_docs = FlatReader().load_data(Path(\"./MockupData/finops.md\"))\n",
        "md_nodes = sentence_spilit.get_nodes_from_documents(md_docs)\n",
        "\n",
        "print(len(md_nodes))\n",
        "index = VectorStoreIndex(md_nodes,embed_model=embed_model)\n",
        "# Query index\n",
        "query_engine = index.as_retriever(similarity_top_k = 5)\n",
        "response = query_engine.retrieve(\"FinOps คืออะไร?\")\n",
        "print('-------------RAG-------------')\n",
        "print(len(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SncGxTHB-0Kc",
        "outputId": "4d224825-7bea-4f7d-b6e4-68dd18d6d3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "-------------RAG-------------\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "separate 2 end point in storage context\n",
        "download from storage to reduce compute time"
      ],
      "metadata": {
        "id": "nE3rIg-P-nx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.vector_stores import SimpleVectorStore\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.core.storage.index_store import SimpleIndexStore\n",
        "from llama_index.core import VectorStoreIndex,StorageContext\n",
        "from llama_index.readers.file import FlatReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from pathlib import Path\n",
        "\n",
        "sentence_spilit = SentenceSplitter(\n",
        "    chunk_size=1024\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=SimpleVectorStore(),\n",
        "    docstore=SimpleDocumentStore(),\n",
        "    index_store=SimpleIndexStore(),\n",
        ")\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\",device='cpu')\n",
        "md_docs = FlatReader().load_data(Path(\"./MockupData/finops.md\"))\n",
        "md_nodes = sentence_spilit.get_nodes_from_documents(md_docs)\n",
        "\n",
        "print(len(md_nodes))\n",
        "index = VectorStoreIndex(md_nodes,embed_model=embed_model,storage_context=storage_context)\n",
        "index.set_index_id(\"FinOps\")\n",
        "index.storage_context.persist(persist_dir=\"./localstore/\")\n",
        "# Query index\n",
        "query_engine = index.as_retriever(similarity_top_k = 5)\n",
        "response = query_engine.retrieve(\"FinOps คืออะไร?\")\n",
        "print('-------------RAG-------------')\n",
        "print(len(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d224825-7bea-4f7d-b6e4-68dd18d6d3b9",
        "id": "rhUuCaHq_Oqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "-------------RAG-------------\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIRWTl61FUm0"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM-as-a-Judge FaithfulnessEvaluator judge llm shouldn't be the same as test_llm"
      ],
      "metadata": {
        "id": "T_oO32tMFD3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVaH31JfFUm3",
        "outputId": "b2a42d0a-9787-476e-cb6b-b944b9fbc312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.vector_stores import SimpleVectorStore\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.core.storage.index_store import SimpleIndexStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "from llama_index.core import VectorStoreIndex,StorageContext\n",
        "from llama_index.readers.file import FlatReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from pathlib import Path\n",
        "\n",
        "# initialize llm\n",
        "test_llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"SeaLLMs/SeaLLM-7B-v2\", token=HF_TOKEN\n",
        ")\n",
        "\n",
        "sentence_spilit = SentenceSplitter(\n",
        "    chunk_size=1024\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=SimpleVectorStore(),\n",
        "    docstore=SimpleDocumentStore(),\n",
        "    index_store=SimpleIndexStore(),\n",
        ")\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\",device='cpu')\n",
        "finops_docs = FlatReader().load_data(Path(\"./MockupData/finops.md\"))\n",
        "\n",
        "finops_nodes = sentence_spilit.get_nodes_from_documents(finops_docs)\n",
        "\n",
        "# create llm\n",
        "llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0.0)\n",
        "\n",
        "# build index\n",
        "finops_index = VectorStoreIndex(finops_nodes,embed_model=embed_model,storage_context=storage_context)\n",
        "\n",
        "# define evaluator\n",
        "evaluator = FaithfulnessEvaluator(llm=llm)\n",
        "\n",
        "# query index\n",
        "query_engine = finops_index.as_query_engine(llm=test_llm)\n",
        "response = query_engine.query(\n",
        "    \"FinOps คืออะไร?\"\n",
        ")\n",
        "eval_result = evaluator.evaluate_response(response=response) # type: ignore\n",
        "print(str(eval_result.passing))\n",
        "print(eval_result.score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Retriever"
      ],
      "metadata": {
        "id": "OnxvNjitFg0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MRR (Mean Reciprocal Rank): The Mean Reciprocal Rank is a metric used to evaluate the effectiveness of a retrieval system based on its ability to return relevant documents. Specifically, it calculates the average of the reciprocal ranks of the first relevant document for a set of queries. The rank is the position of the first relevant document in the list of retrieved documents. The reciprocal rank is the inverse of this position (i.e., 1 divided by the rank). The MRR is the mean (average) of these reciprocal ranks across all queries. This metric is particularly useful when you are interested in the rank of the first relevant result. MRR values range from 0 to 1, where higher values indicate better performance."
      ],
      "metadata": {
        "id": "Ogkf1wqvHV-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hit Rate: Hit Rate is a simpler metric that measures the percentage of queries for which at least one relevant document is retrieved. It's a way to assess the system's ability to retrieve relevant information across a number of queries. Essentially, it checks if a relevant document is present in the retrieved set, without concern for the rank of that document. The Hit Rate is calculated as the number of queries with at least one relevant document retrieved divided by the total number of queries, often expressed as a percentage. Like MRR, higher values of Hit Rate indicate better performance of the retrieval system."
      ],
      "metadata": {
        "id": "-ajKNYJRIKCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Both MRR and Hit Rate are crucial for evaluating the performance of retrieval systems in different contexts. MRR provides insight into the quality of the top-ranked results, emphasizing the importance of ranking relevant documents higher, while Hit Rate gives a broader view of the system's overall ability to retrieve relevant information, regardless of its rank."
      ],
      "metadata": {
        "id": "_9DiVoXyILfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--KNtjjRFUm4"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "\n",
        "retriever = finops_index.as_retriever()\n",
        "\n",
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        ")\n",
        "\n",
        "res = retriever_evaluator.evaluate(\n",
        "    query=\"finops คืออะไร ?\" ,expected_ids=['node_id1']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0oeoI1SFUm5",
        "outputId": "093fbaca-9524-4060-a524-2e5b3b700cfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RetrievalEvalResult(query='finops คืออะไร ?', expected_ids=['node_id1'], expected_texts=None, retrieved_ids=['c23f04c7-9de6-494e-8046-74bb5997cdd6', '719e6b74-4686-4152-903f-cd03c6b081c1'], retrieved_texts=['II FinOps ?  \\nA FinOps (cloud Financial Operations)\\nFinOps Foundation เป็น Foundation ที่ได้รับการสนับสนุนจาก The Linux Foundation ในการช่วยสร้าง Foundation สำหรับการส่งเสริมและผลักดันการใช้งาน Cloud computing อย่างมีคุณค่า\\n\\nB History\\nFinOps Foundation ก่อตั้งขึ้นมาตอน 2019 เกิดขึ้นหลังจากการมีอยู่ของ Cloud computing ประมาณ 10 ปี (AWS establish in 2006) ซึ่งถือว่าเป็น Foundation ที่มีอายุน้อยมาก (เทียบกับปี 2023) ฉะนั้นแล้วในส่วนของ Framework และ Best practice มีการอัพเดทอย่างรวดเร็วและเพิ่มขึ้นอย่างต่อเนื่อง เนื่องจาก use case ส่วนที่ยังไม่ได้ค้นพบยังมีอยู่มหาศาล\\nIII Cloud in real life  \\nการใช้งาน Cloud นั้นเริ่มต้นนั้นจะดูเข้าใจง่าย แต่ความจริงแล้ววิธีคิดเงินของ Cloud นั้นมีความซับซ้อนและความเปลี่ยนแปลงสูง ซึ่งยังมีสิ่งที่ไม่เกี่ยวข้องกับวิธีคิดเงิน แต่เกี่ยวข้องกับความรับผิดชอบบางประการที่ถูกเปลี่ยนแปลงมาโดยไม่มีใครสังเกตุ\\nA Spending decision\\nความรับผิดชอบด้านการจัดซื้อถูกย้ายมาให้กับทีม Engineer โดยตั้งใจหรือไม่ตั้งใจ ก็เป็นไปได้ทั้งสองกรณี โดยปกติแล้วการตัดสินใจในรายจ่ายเราจะมอบให้กับทีม Financial หรือทีม Procurement แต่เมื่อเป็น Cloud environment เรียบร้อยแล้ว คนที่เข้าใจ Cloud มากที่สุดกลับเป็นทีม Engineer ที่ไม่มีหน้าที่ในการรับผิดชอบการใช้จ่ายของบริษัท หรือเมื่อ Spending decision ยังอยู่กับทีม Financial คนที่ต้อง Commit รายจ่ายของ Cloud computing ก็ยังคงเป็นทีม Engineer\\n\\nถ้าไม่แก้ไขปัญหาจากเหตุการณ์ดังกล่าวจะเกิดเหตุการณ์ Engineer Commit รายจ่ายให้สูงเกินความจำเป็น หรือ การใช้งานบางอย่าง (Serverless) ส่งผลให้เกิดรายจ่ายที่ผิดปกติในเดือนนั้น ๆ', 'เครื่องมือ, กิจกรรม, ความเข้ากันได้ หรือ สิ่งขับเคลื่อน ซึ่งแต่ละหัวข้อสามารถแยกพิจารณาเมื่อไหร่ก็ได้ ไม่จำเป็นต้องเรียงลำดับและในแต่ละบริษัทไม่จำที่จะต้องให้น้ำหนักกับแต่ละหัวข้อเหมือนกันทุกบริษัท ขึ้นอยู่กับบริบทของบริษัทนั้น ๆ\\n\\nเราขอเริ่มอธิบายจากหัวข้อแรก\\n\\nA Maturity\\nMaturity เราอาจจะเรียกได้ว่าเป็นประสบการณ์ของทีม แต่ในที่นี้ขอเรียกว่าความเข้ากันได้ เนื่องจากบางบริษัททีมมีความเข้ากันได้อยู่แล้ว เคยทำงานร่วมกันด้วยความเข้าใจดีเยี่ยม ความเข้ากันได้ของทีมนั้น ๆ ก็จะสูงตาม\\n\\nในบริบทของ FinOps เราแบ่ง Maturity ออกเป็น 3 ช่วง ได้แก่\\n\\nCrawl\\nช่วงแรกเริ่มของการนำ FinOps เข้ามาประยุกต์เข้ากับบริษัทของตัวเอง ซึ่งเราสามารถประเมินได้ว่าบริษัทที่อยู่ในช่วงดังกล่าวได้ว่า กระบวนการทำงาน, กระบวนการรายงานผล ยังอยู่ในรูปแบบของ Manual หรือไม่ได้ใช้ระบบอัตโนมัติเข้ามาช่วย รวมถึงการทำงานบางส่วน เช่น Forecast ค่าใช้จ่าย ยังคงต้องคำนวนใหม่ทุกครั้ง ยังไม่ได้สร้างสูตรหรือโมเดลในการช่วยคำนวน ต้องให้ทีม Finance ประเมินทุก ๆ คร้ัง\\n\\nWalk\\nบางบริษัทอาจจะเริ่มจากช่วงนี้เลยก็เป็นไปได้ สำหรับช่วง walk นั้นจะเรียกได้ว่าเป็นช่วงของ Semi-Automate process เนื่องจากทีมมีบุคคลที่มีความสามารถในการสร้าง Automate process ด้วยตัวเอง ไม่ว่าจะเป็นทีม Engineer,'], mode=<RetrievalEvalMode.TEXT: 'text'>, metric_dict={'mrr': RetrievalMetricResult(score=0.0, metadata={}), 'hit_rate': RetrievalMetricResult(score=0.0, metadata={})})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rX7_zRuoUgXm",
        "Qs6tnBL9N_uh",
        "8RVWCnPXEQ3c",
        "DOttCBP5pNRi",
        "BNqkdDB3FCyS",
        "SEJ6XDlPE5ar",
        "3bLGPxi2E5aw",
        "edai5XUOE99j",
        "bkruJPbQFI44",
        "kIRWTl61FUm0"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}